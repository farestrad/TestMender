#!/usr/bin/env python3
"""
analyze_generated_tests.py

Analyze and summarize the test cases generated by Gemini.
Provides statistics and quality metrics.
"""

import json
import sys
from pathlib import Path
from collections import defaultdict


def load_test_file(file_path):
    """Load a test case JSON file."""
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return None


def analyze_test_cases(base_dir="generated_test_cases"):
    """Analyze all generated test cases."""
    base_path = Path(base_dir)
    
    if not base_path.exists():
        print(f"âŒ Directory not found: {base_dir}")
        print("   Run generate_test_cases_gemini.py first")
        sys.exit(1)
    
    print("="*70)
    print("Generated Test Cases Analysis")
    print("="*70)
    
    # Overall statistics
    stats = {
        'total_cwes': 0,
        'total_files': 0,
        'total_test_cases': 0,
        'cwe_breakdown': defaultdict(lambda: {'files': 0, 'tests': 0}),
        'test_case_features': defaultdict(int)
    }
    
    # Process each CWE directory
    cwe_dirs = sorted([d for d in base_path.iterdir() if d.is_dir() and d.name.startswith('CWE')])
    stats['total_cwes'] = len(cwe_dirs)
    
    print(f"\nğŸ“Š Found {len(cwe_dirs)} CWE directories\n")
    
    for cwe_dir in cwe_dirs:
        cwe_name = cwe_dir.name
        print(f"{'='*70}")
        print(f"Analyzing {cwe_name}")
        print(f"{'='*70}")
        
        # Find all test files
        test_files = list(cwe_dir.glob("*_tests.json"))
        stats['cwe_breakdown'][cwe_name]['files'] = len(test_files)
        stats['total_files'] += len(test_files)
        
        print(f"ğŸ“„ Test files: {len(test_files)}")
        
        if len(test_files) == 0:
            print("   âš ï¸  No test files found\n")
            continue
        
        # Analyze each test file
        cwe_test_count = 0
        for test_file in test_files:
            data = load_test_file(test_file)
            if not data or 'test_cases' not in data:
                continue
            
            num_tests = len(data['test_cases'])
            cwe_test_count += num_tests
            stats['total_test_cases'] += num_tests
            
            # Analyze test case features
            for test in data['test_cases']:
                # Check for key fields
                if 'input_data' in test:
                    stats['test_case_features']['has_input_data'] += 1
                if 'bad_expected' in test:
                    stats['test_case_features']['has_bad_expected'] += 1
                if 'good_expected' in test:
                    stats['test_case_features']['has_good_expected'] += 1
                if 'validation_method' in test:
                    stats['test_case_features']['has_validation_method'] += 1
        
        stats['cwe_breakdown'][cwe_name]['tests'] = cwe_test_count
        print(f"ğŸ§ª Test cases: {cwe_test_count}")
        print(f"ğŸ“ˆ Average per file: {cwe_test_count / len(test_files):.1f}\n")
    
    # Print overall summary
    print("="*70)
    print("Overall Summary")
    print("="*70)
    print(f"âœ… Total CWEs processed: {stats['total_cwes']}")
    print(f"âœ… Total files processed: {stats['total_files']}")
    print(f"âœ… Total test cases generated: {stats['total_test_cases']}")
    
    if stats['total_files'] > 0:
        print(f"ğŸ“Š Average test cases per file: {stats['total_test_cases'] / stats['total_files']:.1f}")
    
    # CWE breakdown
    print(f"\n{'='*70}")
    print("Per-CWE Breakdown")
    print(f"{'='*70}")
    print(f"{'CWE':<15} {'Files':<10} {'Tests':<10} {'Avg/File':<10}")
    print("-"*70)
    
    for cwe_name in sorted(stats['cwe_breakdown'].keys()):
        data = stats['cwe_breakdown'][cwe_name]
        avg = data['tests'] / data['files'] if data['files'] > 0 else 0
        print(f"{cwe_name:<15} {data['files']:<10} {data['tests']:<10} {avg:<10.1f}")
    
    # Test case quality metrics
    print(f"\n{'='*70}")
    print("Test Case Quality Metrics")
    print(f"{'='*70}")
    
    total_tests = stats['total_test_cases']
    if total_tests > 0:
        print(f"Test cases with input_data:        {stats['test_case_features']['has_input_data']:>6} ({100*stats['test_case_features']['has_input_data']/total_tests:.1f}%)")
        print(f"Test cases with bad_expected:      {stats['test_case_features']['has_bad_expected']:>6} ({100*stats['test_case_features']['has_bad_expected']/total_tests:.1f}%)")
        print(f"Test cases with good_expected:     {stats['test_case_features']['has_good_expected']:>6} ({100*stats['test_case_features']['has_good_expected']/total_tests:.1f}%)")
        print(f"Test cases with validation_method: {stats['test_case_features']['has_validation_method']:>6} ({100*stats['test_case_features']['has_validation_method']/total_tests:.1f}%)")
    
    # Sample test cases
    print(f"\n{'='*70}")
    print("Sample Test Cases")
    print(f"{'='*70}")
    
    samples_shown = 0
    for cwe_dir in cwe_dirs[:2]:  # Show samples from first 2 CWEs
        test_files = list(cwe_dir.glob("*_tests.json"))
        if test_files:
            sample_file = test_files[0]
            data = load_test_file(sample_file)
            if data and 'test_cases' in data and len(data['test_cases']) > 0:
                print(f"\nğŸ“„ From {sample_file.name}:")
                test = data['test_cases'][0]
                print(f"   Test ID: {test.get('test_id', 'N/A')}")
                print(f"   Description: {test.get('description', 'N/A')}")
                print(f"   Input: {test.get('input_data', 'N/A')[:80]}...")
                print(f"   Bad Expected: {test.get('bad_expected', 'N/A')[:80]}...")
                print(f"   Good Expected: {test.get('good_expected', 'N/A')[:80]}...")
                samples_shown += 1
    
    print(f"\n{'='*70}")
    print("âœ… Analysis complete!")
    print(f"{'='*70}")
    
    # Save analysis results
    output_file = Path(base_dir) / "analysis_summary.json"
    with open(output_file, 'w') as f:
        # Convert defaultdict to regular dict for JSON serialization
        stats_copy = {
            'total_cwes': stats['total_cwes'],
            'total_files': stats['total_files'],
            'total_test_cases': stats['total_test_cases'],
            'cwe_breakdown': dict(stats['cwe_breakdown']),
            'test_case_features': dict(stats['test_case_features'])
        }
        json.dump(stats_copy, f, indent=2)
    
    print(f"\nğŸ’¾ Analysis saved to: {output_file}")


def show_detailed_test(file_path):
    """Show detailed view of a specific test file."""
    data = load_test_file(file_path)
    if not data:
        return
    
    print("="*70)
    print(f"Test File: {file_path.name}")
    print("="*70)
    print(f"Source File: {data.get('file_name', 'N/A')}")
    print(f"Good Functions: {', '.join(data.get('good_functions', []))}")
    print(f"Bad Functions: {', '.join(data.get('bad_functions', []))}")
    print(f"\nTest Cases: {len(data.get('test_cases', []))}")
    
    for i, test in enumerate(data.get('test_cases', []), 1):
        print(f"\n{'â”€'*70}")
        print(f"Test Case #{i}")
        print(f"{'â”€'*70}")
        print(f"Description: {test.get('description', 'N/A')}")
        print(f"\nInput Data:")
        print(f"  {test.get('input_data', 'N/A')}")
        print(f"\nBad Version Expected:")
        print(f"  {test.get('bad_expected', 'N/A')}")
        print(f"\nGood Version Expected:")
        print(f"  {test.get('good_expected', 'N/A')}")
        print(f"\nValidation Method:")
        print(f"  {test.get('validation_method', 'N/A')}")


def main():
    """Main function."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Analyze generated test cases')
    parser.add_argument('--base-dir', default='generated_test_cases',
                       help='Base directory containing generated test cases')
    parser.add_argument('--detail', type=str,
                       help='Show detailed view of a specific test file')
    
    args = parser.parse_args()
    
    if args.detail:
        detail_path = Path(args.detail)
        if detail_path.exists():
            show_detailed_test(detail_path)
        else:
            print(f"âŒ File not found: {args.detail}")
    else:
        analyze_test_cases(args.base_dir)


if __name__ == "__main__":
    main()